{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "NAMA : Mohammad Nurdin Prastya Hermansah\n",
        "\n",
        "Prodi : D4 Teknik Elektronika\n",
        "\n",
        "NIM: 20507334047"
      ],
      "metadata": {
        "id": "9ZloHEQS42He"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIGnXkepKExV"
      },
      "source": [
        "# Machine Learning Exercise 3 - Multi-Class Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NsbQfXUBKExh"
      },
      "source": [
        "For this exercise we'll use logistic regression to recognize hand-written digits (0 to 9).  We'll be extending the implementation of logistic regression we wrote in exercise 2 and apply it to one-vs-all classification.  Let's get started by loading the data set.  It's in MATLAB's native format, so to load it in Python we need to use a SciPy utility.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Pada latihan ini kita akan menggunakan logistic regression to mengenali tulisan tangan digits(0-9). Kita akan memperluas penerapan regresi logistik yang kita tulis di latihan 2 dan menerapkannya pada klasifikasi one-vs-all. Mari kita mulai dengan memuat kumpulan data. Itu dalam format asli MATLAB, jadi untuk memuatnya dengan Python kita perlu menggunakan utilitas SciPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CoQeVb4gKExj"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b49VkHtdKh2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dfd193-2a46-41f4-b3c1-26e61d83ddfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# membaca data dan menampilkanya\n",
        "data = loadmat('/content/drive/MyDrive/PRAKTEK/Data Praktek/ex3data.mat')\n",
        "data['X'].shape, data['y'].shape"
      ],
      "metadata": {
        "id": "YcY7xLvsLQYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346152d3-8291-4c49-a01d-0818784b3079"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 400), (5000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-45Rrp3KExq"
      },
      "source": [
        "Great, we've got our data loaded.  The images are represented in matrix X as a 400-dimensional vector (of which there are 5,000 of them).  The 400 \"features\" are grayscale intensities of each pixel in the original 20 x 20 image.  The class labels are in the vector y as a numeric class representing the digit that's in the image.\n",
        "\n",
        "The first task is to modify our logistic regression implementation to be completely vectorized (i.e. no \"for\" loops).  This is because vectorized code, in addition to being short and concise, is able to take advantage of linear algebra optimizations and is typically much faster than iterative code.  However if you look at our cost function implementation from exercise 2, it's already vectorized!  So we can re-use the same implementation here.  Note we're skipping straight to the final, regularized version."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artinya**\n",
        "\n",
        "Bagus, data kita sudah dimuat. Gambar direpresentasikan dalam matriks X sebagai vektor 400 dimensi (yang jumlahnya 5.000). 400 \"fitur\" adalah intensitas skala abu-abu dari setiap piksel dalam gambar asli 20 x 20. Label kelas ada di vektor y sebagai kelas numerik yang mewakili digit yang ada di gambar.\n",
        "\n",
        "Tugas pertama adalah memodifikasi implementasi regresi logistik kita menjadi sepenuhnya vektor (i.e. no \"for\" loops). Ini karena kode vektor, selain pendek dan ringkas, mampu memanfaatkan pengoptimalan aljabar linier dan biasanya jauh lebih cepat daripada kode iteratif. Namun jika Anda melihat implementasi fungsi cost kita dari latihan 2, itu sudah di-vektor-kan! Jadi kita dapat menggunakan kembali implementasi yang sama di sini. Perhatikan bahwa kami langsung melompat ke versi final yang telah diatur."
      ],
      "metadata": {
        "id": "qZuTSmKU8o0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah pertama kita perlu membuat fungsi sigmoid. Kode untuk ini cukup sederhana."
      ],
      "metadata": {
        "id": "RHft6Yrp9-2F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "34KPSY-6KExr"
      },
      "outputs": [],
      "source": [
        "# function sigmoid\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sekarang kita harus menulis fungsi cost untuk mengevaluasi solusi."
      ],
      "metadata": {
        "id": "4q6gP9zW-Irs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "xOKXelKAKExt"
      },
      "outputs": [],
      "source": [
        "# evaluate solution\n",
        "def cost(theta, X, y, learningRate):\n",
        "    theta = np.matrix(theta)\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
        "    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
        "    reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))\n",
        "    return np.sum(first - second) / (len(X)) + reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhsLQH_vKExv"
      },
      "source": [
        "Next we need the function that computes the gradient.  Again, we already defined this in the previous exercise, only in this case we do have a \"for\" loop in the update step that we need to get rid of.  Here's the original code for reference:\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Selanjutnya kita membutuhkan fungsi yang menghitung gradien. Sekali lagi, kita telah mendefinisikan ini pada latihan sebelumnya, hanya dalam kasus ini kita memiliki loop \"untuk\" pada langkah pembaruan yang perlu kita singkirkan. Berikut kode asli untuk referensi:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "nkGB2IggKExx"
      },
      "outputs": [],
      "source": [
        "# computing gradient with \"for\" loop in the update step that we need to get rid of.\n",
        "def gradient_with_loop(theta, X, y, learningRate):\n",
        "    theta = np.matrix(theta)\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    parameters = int(theta.ravel().shape[1])\n",
        "    grad = np.zeros(parameters)\n",
        "    \n",
        "    error = sigmoid(X * theta.T) - y\n",
        "    \n",
        "    for i in range(parameters):\n",
        "        term = np.multiply(error, X[:,i])\n",
        "        \n",
        "        if (i == 0):\n",
        "            grad[i] = np.sum(term) / len(X)\n",
        "        else:\n",
        "            grad[i] = (np.sum(term) / len(X)) + ((learningRate / len(X)) * theta[:,i])\n",
        "    \n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrJNF0PvKExz"
      },
      "source": [
        "In our new version we're going to pull out the \"for\" loop and compute the gradient for each parameter at once using linear algebra (except for the intercept parameter, which is not regularized so it's computed separately).  To follow the math behind the transformation, refer to the exercise 3 text.\n",
        "\n",
        "Also note that we're converting the data structures to NumPy matrices. This is done in an attempt to make the code look more similar to Octave than it would using arrays because matrices automatically follow matrix operation rules vs. element-wise operations, which is the default for arrays.  There is some debate in the community over wether or not the matrix class should be used at all, but it's there so we're using it in these examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artinya**\n",
        "\n",
        "Dalam versi baru kami, kami akan mengeluarkan loop \"untuk\" dan menghitung gradien untuk setiap parameter sekaligus menggunakan aljabar linier (kecuali untuk parameter intersep, yang tidak diatur sehingga dihitung secara terpisah). Untuk mengikuti matematika di balik transformasi, lihat teks latihan 3.\n",
        "\n",
        "Perhatikan juga bahwa kami mengonversi struktur data menjadi matriks NumPy. Ini dilakukan dalam upaya untuk membuat kode terlihat lebih mirip dengan Oktaf daripada menggunakan array karena matriks secara otomatis mengikuti aturan operasi matriks vs. operasi berdasarkan elemen, yang merupakan default untuk array. Ada beberapa perdebatan di komunitas tentang apakah kelas matriks harus digunakan atau tidak, tetapi itu ada di sana, jadi kami menggunakannya dalam contoh ini.\n"
      ],
      "metadata": {
        "id": "fs9vpChD_FmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "o4a1ONLRKEx0"
      },
      "outputs": [],
      "source": [
        "#  menghitung gradien \n",
        "def gradient(theta, X, y, learningRate):\n",
        "    theta = np.matrix(theta)\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "    \n",
        "    parameters = int(theta.ravel().shape[1])\n",
        "    error = sigmoid(X * theta.T) - y\n",
        "    \n",
        "    grad = ((X.T * error) / len(X)).T + ((learningRate / len(X)) * theta)\n",
        "    \n",
        "    # intercept gradient is not regularized\n",
        "    grad[0, 0] = np.sum(np.multiply(error, X[:,0])) / len(X)\n",
        "    \n",
        "    return np.array(grad).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-38G0yWKEx1"
      },
      "source": [
        "Now that we've defined our cost and gradient functions, it's time to build a classifier.  For this task we've got 10 possible classes, and since logistic regression is only able to distiguish between 2 classes at a time, we need a strategy to deal with the multi-class scenario.  In this exercise we're tasked with implementing a one-vs-all classification approach, where a label with k different classes results in k classifiers, each one deciding between \"class i\" and \"not class i\".  We're going to wrap the classifier training up in one function that computes the final weights for each of the 10 classifiers and returns the weights as a k X (n + 1) array, where n is the number of parameters.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Sekarang setelah kita menentukan fungsi cost dan gradien, saatnya membuat classifier. Untuk tugas ini, kami memiliki 10 kemungkinan kelas, dan karena regresi logistik hanya dapat membedakan antara 2 kelas sekaligus, kami memerlukan strategi untuk menghadapi skenario multi-kelas. Dalam latihan ini kita ditugaskan untuk mengimplementasikan pendekatan klasifikasi satu lawan semua, di mana sebuah label dengan k kelas yang berbeda menghasilkan k pengklasifikasi, masing-masing memutuskan antara \"kelas i\" dan \"bukan kelas i\". Kita akan membungkus pelatihan pengklasifikasi dalam satu fungsi yang menghitung bobot akhir untuk masing-masing dari 10 pengklasifikasi dan mengembalikan bobot sebagai larik k X (n + 1), di mana n adalah jumlah parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "rIcaHb0hKEx1"
      },
      "outputs": [],
      "source": [
        "# import library scipy\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# membuat classifier\n",
        "def one_vs_all(X, y, num_labels, learning_rate):\n",
        "    rows = X.shape[0]\n",
        "    params = X.shape[1]\n",
        "    \n",
        "    # k X (n + 1) array for the parameters of each of the k classifiers\n",
        "    all_theta = np.zeros((num_labels, params + 1))\n",
        "    \n",
        "    # insert a column of ones at the beginning for the intercept term\n",
        "    X = np.insert(X, 0, values=np.ones(rows), axis=1)\n",
        "    \n",
        "    # labels are 1-indexed instead of 0-indexed\n",
        "    for i in range(1, num_labels + 1):\n",
        "        theta = np.zeros(params + 1)\n",
        "        y_i = np.array([1 if label == i else 0 for label in y])\n",
        "        y_i = np.reshape(y_i, (rows, 1))\n",
        "        \n",
        "        # minimize the objective function\n",
        "        fmin = minimize(fun=cost, x0=theta, args=(X, y_i, learning_rate), method='TNC', jac=gradient)\n",
        "        all_theta[i-1,:] = fmin.x\n",
        "    \n",
        "    return all_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ebyz2ftKEx3"
      },
      "source": [
        "A few things to note here...first, we're adding an extra parameter to theta (along with a column of ones to the training data) to account for the intercept term.  Second, we're transforming y from a class label to a binary value for each classifier (either is class i or is not class i).  Finally, we're using SciPy's newer optimization API to minimize the cost function for each classifier.  The API takes an objective function, an initial set of parameters, an optimization method, and a jacobian (gradient) function if specified.  The parameters found by the optimization routine are then assigned to the parameter array.\n",
        "\n",
        "One of the more challenging parts of implementing vectorized code is getting all of the matrix interactions written correctly, so I find it useful to do some sanity checks by looking at the shapes of the arrays/matrices I'm working with and convincing myself that they're sensible.  Let's look at some of the data structures used in the above function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artinya**\n",
        "\n",
        "Beberapa hal yang perlu diperhatikan di sini ... pertama, kami menambahkan parameter tambahan ke theta (bersama dengan kolom satu ke data pelatihan) untuk memperhitungkan istilah intersep. Kedua, kami mengubah y dari label kelas menjadi nilai biner untuk setiap pengklasifikasi (baik kelas i atau bukan kelas i). Terakhir, kami menggunakan API pengoptimalan SciPy yang lebih baru untuk meminimalkan fungsi biaya untuk setiap pengklasifikasi. API mengambil fungsi objektif, kumpulan parameter awal, metode pengoptimalan, dan fungsi jacobian (gradien) jika ditentukan. Parameter yang ditemukan oleh rutin optimasi kemudian ditugaskan ke array parameter.\n",
        "\n",
        "Salah satu bagian yang lebih menantang dalam mengimplementasikan kode vektorisasi adalah membuat semua interaksi matriks ditulis dengan benar, jadi saya merasa berguna untuk melakukan beberapa pemeriksaan kewarasan dengan melihat bentuk array/matriks yang saya kerjakan dan meyakinkan diri sendiri bahwa mereka masuk akal. Mari kita lihat beberapa struktur data yang digunakan dalam fungsi di atas."
      ],
      "metadata": {
        "id": "Te8h9MzBAo0w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ipTaoR84KEx3",
        "outputId": "7fc08c5b-9a39-4e96-d432-687347123285",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 401), (5000, 1), (401,), (10, 401))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "rows = data['X'].shape[0]\n",
        "params = data['X'].shape[1]\n",
        "\n",
        "all_theta = np.zeros((10, params + 1))\n",
        "\n",
        "X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)\n",
        "\n",
        "theta = np.zeros(params + 1)\n",
        "\n",
        "y_0 = np.array([1 if label == 0 else 0 for label in data['y']])\n",
        "y_0 = np.reshape(y_0, (rows, 1))\n",
        "\n",
        "X.shape, y_0.shape, theta.shape, all_theta.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVlOfUt4KEx4"
      },
      "source": [
        "These all appear to make sense.  Note that theta is a one-dimensional array, so when it gets converted to a matrix in the code that computes the gradient, it turns into a (1 X 401) matrix.  Let's also check the class labels in y to make sure they look like what we're expecting.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Ini semua tampaknya masuk akal. Perhatikan bahwa theta adalah larik satu dimensi, jadi ketika diubah menjadi matriks dalam kode yang menghitung gradien, teta berubah menjadi matriks (1 X 401). Mari kita periksa juga label kelas di y untuk memastikannya terlihat seperti yang kita harapkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rOUT8kD7KEx5",
        "outputId": "03579f0b-15bd-462f-be39-9633f9e9d0e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "np.unique(data['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPh1-dMfKEx7"
      },
      "source": [
        "Let's make sure that our training function actually runs, and we get some sensible outputs, before going any further.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Mari pastikan bahwa fungsi pelatihan kita benar-benar berjalan, dan kita mendapatkan hasil yang masuk akal, sebelum melangkah lebih jauh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qa6a41TmKEx7",
        "outputId": "6fd1689f-a15a-479e-c2be-e14f324edc9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.70247931e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "        -2.24803602e-10,  2.31962907e-11,  0.00000000e+00],\n",
              "       [-8.96250716e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         7.26120810e-09, -6.19965284e-10,  0.00000000e+00],\n",
              "       [-8.39553387e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "        -7.61695633e-10,  4.64917656e-11,  0.00000000e+00],\n",
              "       ...,\n",
              "       [-7.00832439e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "        -6.92009029e-10,  4.29241494e-11,  0.00000000e+00],\n",
              "       [-7.65187941e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "        -8.09503274e-10,  5.31058721e-11,  0.00000000e+00],\n",
              "       [-6.63412747e-05,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "        -3.49766172e-09,  1.13668635e-10,  0.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "all_theta = one_vs_all(data['X'], data['y'], 10, 1)\n",
        "all_theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULdvtucKEx8"
      },
      "source": [
        "We're now ready for the final step - using the trained classifiers to predict a label for each image.  For this step we're going to compute the class probability for each class, for each training instance (using vectorized code of course!) and assign the output class label as the class with the highest probability.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Kami sekarang siap untuk langkah terakhir - menggunakan pengklasifikasi terlatih untuk memprediksi label untuk setiap gambar. Untuk langkah ini kita akan menghitung probabilitas kelas untuk setiap kelas, untuk setiap instance pelatihan (tentu saja menggunakan kode vektor!) dan menetapkan label kelas keluaran sebagai kelas dengan probabilitas tertinggi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "qmBEMQIhKEx-"
      },
      "outputs": [],
      "source": [
        "def predict_all(X, all_theta):\n",
        "    rows = X.shape[0]\n",
        "    params = X.shape[1]\n",
        "    num_labels = all_theta.shape[0]\n",
        "    \n",
        "    # same as before, insert ones to match the shape\n",
        "    X = np.insert(X, 0, values=np.ones(rows), axis=1)\n",
        "    \n",
        "    # convert to matrices\n",
        "    X = np.matrix(X)\n",
        "    all_theta = np.matrix(all_theta)\n",
        "    \n",
        "    # compute the class probability for each class on each training instance\n",
        "    h = sigmoid(X * all_theta.T)\n",
        "    \n",
        "    # create array of the index with the maximum probability\n",
        "    h_argmax = np.argmax(h, axis=1)\n",
        "    \n",
        "    # because our array was zero-indexed we need to add one for the true label prediction\n",
        "    h_argmax = h_argmax + 1\n",
        "    \n",
        "    return h_argmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls18_yjEKEx_"
      },
      "source": [
        "Now we can use the predict_all function to generate class predictions for each instance and see how well our classifier works.\n",
        "\n",
        "**Artinya**\n",
        "\n",
        "Sekarang kita dapat menggunakan fungsi predict_all untuk menghasilkan prediksi kelas untuk setiap instance dan melihat seberapa baik classifier kita bekerja."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ibX8x87VKEyD",
        "outputId": "5e713c5d-193d-4910-86b0-f8686a3f5bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 74.6%\n"
          ]
        }
      ],
      "source": [
        "y_pred = predict_all(data['X'], all_theta)\n",
        "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kesimpulan**\n",
        "\n",
        "Dalam melakukan prediksi Multi-Class Classification hal yang harus dilakukan adalah:\n",
        "1. import library\n",
        "2. import data\n",
        "3. pembacaan dan penampilan data\n",
        "4. membuat fungsi sigmoid\n",
        "5. sekarang kita harus menulis fungsi cost untuk mengevaluasi solusi.\n",
        "6. menghitung gradiet\n",
        "7. membuat classifier \n",
        "  - k X (n + 1) array for the parameters of each of the k classifiers\n",
        "  - insert a column of ones at the beginning for the intercept term\n",
        "  - labels are 1-indexed instead of 0-indexed\n",
        "  - minimize the objective function\n",
        "8. theta adalah larik satu dimensi diubah menjadi matriks dalam kode yang menghitung gradien.\n",
        "9. melakuakan cheking fungsi\n",
        "10.  predict a label for each image\n",
        "  - same as before, insert ones to match the shape\n",
        "  - convert to matrices\n",
        "  - compute the class probability for each class on each training instance\n",
        "  - create array of the index with the maximum probability\n",
        "  - because our array was zero-indexed we need to add one for the true label prediction\n",
        "11. gunakan fungsi predict_all untuk menghasilkan prediksi kelas untuk setiap instance dan melihat seberapa baik classifier kita bekerja."
      ],
      "metadata": {
        "id": "ApZLryUED9kV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}